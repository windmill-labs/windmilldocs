---
slug: ai-agents
authors: [farhadgoulamabasse, hugocasa]
tags: ['ai', 'windmill', 'agents']
description: 'How AI agent steps work in Windmill: multi-provider support, tool integration, and handling structured output across providers'
title: 'AI agent steps in Windmill flows'
image: './thumbnail.png'
---

import chatModeVideo from './chat_mode.mp4';
import demoVideo from './demo.mp4';

# AI agent steps in Windmill flows

The rise of large language models has created a new paradigm for workflow automation: instead of predefining every branch and decision in your [workflows](/docs/flows/flow_editor), you can let an AI agent reason about which tools to use and orchestrate them dynamically based on context.

[AI agent steps](/docs/core_concepts/ai_agents) in Windmill bring this capability to your workflows. Define the tools available, a Windmill [script](/docs/getting_started/scripts_quickstart) or any tools exposed by an MCP server. Then let the agent decide which to call, when to call them, and how to combine their results. The agent becomes a flexible orchestrator that adapts to each request rather than following a rigid script.

This post explores what AI agent steps enable for Windmill users, then dives into two technical challenges we solved: making structured output work consistently across different AI providers, and maintaining MCP protocol compliance as the ecosystem matures.

<!--truncate-->

## What AI agent steps bring to Windmill

<video
    controls
    autoPlay
    muted
    loop
    playsInline
    className="w-full rounded-lg border-2 dark:border-gray-800 my-4"
        onLoadedMetadata={(e) => {
        e.currentTarget.playbackRate = 1.5;
    }}
    onPlay={(e) => {
        e.currentTarget.playbackRate = 1.5;
    }}

>
    <source src={demoVideo} />
    Your browser does not support the video tag.
    <a href={demoVideo}>Download the video</a>.
</video>

### Tool integration

Any Windmill [script](/docs/getting_started/scripts_quickstart) becomes a tool the AI agent can invoke. Write your tools in any of the 10+ languages Windmill supportsâ€”Python, TypeScript, Go, Rust, PHP, Bash, SQL, and more. The agent examines each tool's schema, understands its capabilities, and reasons about which tools to use based on the user's request. You can also use tools from the [Windmill Hub](https://hub.windmill.dev).

![Script tools](./script_tool.png "Define your tool in any language")

### Conversational workflows

Enable Chat Mode in your [flow](/docs/flows/flow_editor), and Windmill transforms your workflow into a conversational experience. Instead of traditional form inputs, users interact through natural conversation.

This works through two key mechanisms. Conversation memory keeps context across the entire interaction, the agent remembers earlier messages and builds on previous exchanges. Configure how much history to maintain, and the agent will recall relevant information throughout the conversation, understanding the broader context of what you're trying to accomplish rather than treating each message in isolation.

Streaming makes the agent's work transparent. As the agent calls tools, processes results, and formulates responses, users can see real-time updates showing exactly what's happening. This visibility is particularly useful for complex workflows where the agent might call multiple tools sequentially, you can follow along rather than staring at a loading spinner.

The result is a workflow that feels truly conversational: the agent maintains context like a human would, and users can see its reasoning unfold in real-time.

<video
    controls
    autoPlay
    muted
    loop
    playsInline
    className="w-full rounded-lg border-2 dark:border-gray-800 my-4"
        onLoadedMetadata={(e) => {
        e.currentTarget.playbackRate = 1.5;
    }}
    onPlay={(e) => {
        e.currentTarget.playbackRate = 1.5;
    }}

>
    <source src={chatModeVideo} />
    Your browser does not support the video tag.
    <a href={chatModeVideo}>Download the video</a>.
</video>

### Multi-provider support and configuration

Configure your AI agent with any AI provider: [OpenAI](https://openai.com/), [Anthropic](https://www.anthropic.com/), [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/overview), [Mistral](https://mistral.ai/), [Google AI](https://ai.google.dev/models/gemini), [Groq](https://groq.com/), [Together AI](https://together.ai/), [OpenRouter](https://openrouter.ai/), or any custom or local endpoint you operate.

Fine-tune your agent's behavior with configuration options: set system prompts to guide how the agent approaches tasks, adjust temperature to control creativity versus consistency, and set maximum output tokens to manage costs.

### Additional capabilities

**Structured output**: Conversational text is useful, but sometimes you need data in a specific format that downstream systems can consume reliably. With [JSON schema](/docs/core_concepts/json_schema_and_parsing) validation, you can ensure the AI's response conforms to a precise structure, returning a standardized object rather than free-form text.

**Image support**: AI agent steps support images as both input and output. Provide images for the agent to analyze, or have the agent generate images in response to your request. Generated images are automatically stored in your workspace's S3-compatible [object storage](/docs/core_concepts/object_storage_in_windmill), making them immediately available for subsequent workflow steps.

![Image output](./image_output.png "Generate images seamlessly")

**MCP integration**: Through [Model Context Protocol](https://modelcontextprotocol.io/docs/getting-started/intro) support, agents can connect to external MCP servers: file system browsers, database interfaces, API integrations, and custom business logic servers. This extends the agent beyond Windmill's internal capabilities to any MCP-compatible service.

![MCP tool](./mcp.png "Connect to MCP servers")

## Technical challenges

Building AI agent steps meant solving real technical challenges. Two stand out: making structured output work consistently across providers with different capabilities, and maintaining protocol compliance in a maturing ecosystem.

### Structured output across providers

Supporting multiple AI providers reveals an ongoing challenge: many providers claim OpenAI compatibility, but real-world differences require effort to handle. Structured output illustrates this well.

Most providers support structured output through a response_format parameter. You specify a JSON schema, and the model ensures its response conforms to that structure. This works straightforwardly for OpenAI, Mistral, Google AI, and several other providers.

Anthropic's models don't support response_format. Rather than limiting functionality for Anthropic users, we implemented a workaround: define a special tool where the tool's input schema matches the desired output structure. The agent calls this tool as its final action, and the tool's arguments become the structured response.

From the user's perspective, structured output works uniformly across all providers. The implementation differs behind the scenes, but the interface remains consistent. This approach lets us support providers with different capabilities while maintaining a unified experience.

### MCP protocol compliance

Windmill uses the official [rmcp](https://github.com/modelcontextprotocol/rust-sdk) Rust crate for MCP support. This is a well-engineered implementation that strictly follows the MCP protocol specification, exactly as it should.

However, MCP is still a young protocol. As the ecosystem develops, we've encountered servers that don't implement the specification precisely. These aren't malicious implementations, they're often early versions or experimental servers where the authors interpreted certain edge cases differently than the spec intended.

The types of issues that arise typically involve:

- Incorrect HTTP status codes in error responses
- Deviations in how servers signal unsupported features
- Inconsistent handling of optional protocol elements
- Subtle differences in message format expectations

When Windmill connects to a non-compliant server, the strict protocol implementation in rmcp correctly rejects the connection rather than trying to work around the deviation.
This is a bet on the ecosystem's long-term health. By maintaining strict compliance, we provide clear error messages about what's wrong and create incentives for servers to fix protocol issues. As the MCP ecosystem matures, these compatibility problems should hopefully diminish.

## A natural fit for workflow orchestration

[AI agent steps](/docs/core_concepts/ai_agents) in Windmill aren't a separate system grafted onto the platform, they're a natural extension of what Windmill already does well. By building on Windmill's existing [workflow engine](/docs/flows/flow_editor), multi-language support, and schema-first design, we created a feature that feels native because it truly is.

The result is a system where AI agents orchestrate workflows the same way humans do: by calling tools, processing results, and making decisions based on context. The tools happen to be Windmill [scripts](/docs/getting_started/scripts_quickstart) in any language. The execution happens through the same job queue that runs every other workflow. The storage uses the same [S3 integration](/docs/core_concepts/object_storage_in_windmill) that handles all large artifacts.

As the AI ecosystem evolves, Windmill's AI agent steps will evolve with it. Not because we're constantly rebuilding, but because we built on solid foundations from the start.