import DocCard from '@site/src/components/DocCard';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Persistent Storage

Persistent storage refers to any method of storing data that remains intact and accessible even after a system is powered off, restarted, or experiences a crash.

In the context of Windmill, the stakes are: **where to effectively store and manage the data manipulated by Windmill** (ETL, data ingestion and preprocessing, data migration and sync etc.) ?

:::info TLDR

When it comes to storing data manipulated by Windmil, it is recommended to only store Windmill-specific elements ([resources](../3_resources_and_types/index.mdx), [variables](../2_variables_and_secrets/index.mdx) etc.). To store data, it is recommended to use external storage service providers that can be accessed from Windmill.

<br />

This present document gives a list of trusted services to use alongside Windmill.

:::

<br />

There are 4 kinds of persistent storage in Windmill:

1. [Small data](#within-windmill-not-recommended) that is relevant in between script/flow execution and can be persisted on Windmill itself.

2. [Big structured SQL data](#structured-databases-postgres-supabase-neontech) that is critical to your services and that is stored externally on an SQL Database or Data Warehouse.

3. [Object storage for large data](#large-data-files-s3-r2-minio) such as S3.

4. [NoSQL and document database](#key-value-stores-mongodb-atlas-redis-upstash) such as MongoDB and Key-Value stores.

## You already have your own database

If you already have your own database provided by a supported integration, you can easily connect it to Windmill.

If your service provider is already part of [our list of integrations](../../integrations/0_integrations_on_windmill.mdx), just add your database as a [resource](../../core_concepts/3_resources_and_types/index.mdx).

"If your service provider is not already integrated with Windmill, you can create a [new resource type](../../core_concepts/3_resources_and_types/index.mdx#create-a-resource-type) to establish the connection (and if you want, [share the schema on our Hub](../../misc/1_share_on_hub/index.md)).

## Within Windmill: not recommended

Windmill is not designed to store heavy data that extends beyond the execution of a script or flow. Indeed, for each computation the worker executing is not the same as the previous computation, so the data would have to be retrieved from another location.

Instead, Windmill is very convenient to use alongside data storage providers to manipulate big amounts of data.

There are however internal methods to persist data between executions of jobs.

### States and Resources

Within Windmill, you can use States and Resources as a way to store a transient state - that can be represented as small JSON.

#### States

States are used by scripts to keep data persistent between runs of the same script by the same trigger (schedule or user).

In Windmill, [States](../3_resources_and_types/index.mdx#states) are considered as resources, but they are excluded from the Workspace tab for clarity.
They are displayed on the Resources menu, under a dedicated tab.

A state is an object stored as a resource of the resource type `state` which is meant to persist across distinct executions of the same script.

```py
import requests
from wmill import set_state, get_state

def main():
	# Get temperature from last execution
    last_temperature = get_state()

    # Fetch the temperature in Paris from wttr.in
    response = requests.get("http://wttr.in/Paris?format=%t")

    new_temperature = response.text.strip("Â°F")

	# Set current temperature to state
    set_state(new_temperature)

    # Compare last_temperature and new_temperature
    if last_temperature < new_temperature:
        return "The temperature has increased."
    elif last_temperature > new_temperature:
        return "The temperature has decreased."
    else:
        return "The temperature has remained the same."
```

States are what enable Flows to watch for changes in most event watching scenarios ([trigger scripts](../../flows/10_flow_trigger.mdx)). The pattern is as follows:

- Retrieve the last state or, if undefined, assume it is the first execution.
- Retrieve the current state in the external system you are watching, e.g. the
  list of users having starred your repo or the maximum ID of posts on Hacker News.
- Calculate the difference between the current state and the last internal
  state. This difference is what you will want to act upon.
- Set the new state as the current state so that you do not process the
  elements you just processed.
- Return the differences calculated previously so that you can process them in
  the next steps. You will likely want to [forloop](../../flows/12_flow_loops.md) over the items and trigger
  one Flow per item. This is exactly the pattern used when your Flow is in the
  mode of "Watching changes regularly".

The convenience functions do this are:

_TypeScript_

- `getState()` which retrieves an object of any type (internally a simple
  Resource) at a path determined by `getStatePath`, which is unique to the user
  currently executing the Script, the Flow in which it is currently getting
  called in - if any - and the path of the Script.
- `setState(value: any)` which sets the new state.

> Please note it requires [importing](../../advanced/6_imports/index.md) the wmill client library from Deno/Bun.

<br />

_Python_

- `get_state()` which retrieves an object of any type (internally a simple
  Resource) at a path determined by `get_state_path`, which is unique to the user
  currently executing the Script, the Flow in which it is currently getting
  called in - if any - and the path of the Script.
- `set_state(value: Any)` which sets the new state.

> Please note it requires [importing](../../advanced/6_imports/index.md) the wmill client library from Python.

<br />

<div class="grid grid-cols-2 gap-6 mb-4">
	<DocCard
		title="States"
		description="A state is an object stored as a resource of the resource type `state` which is meant to persist across distinct executions of the same script."
		href="/docs/core_concepts/resources_and_types#states"
	/>
</div>

#### Resources

[States](#states) are a specific type of [resources](../3_resources_and_types/index.mdx) in Windmill where the type is `state` the path is automatically calculated for you based on the schedule path (if any) and the script path. In some cases, you want to set the path arbitrarily and/or use a different type than `state`. In this case, you can use the `setResource` and `getResource` functions. A same resource can be used across different scripts and flows.

- `setResource(value: any, path?: string, initializeToTypeIfNotExist?: string)`: which sets a resource at a given path. This is
  equivalent to `setState` but allows you to set an arbitrary path and chose a type other than state if wanted. [See API](https://deno.land/x/windmill/mod.ts?s=setResource).
- `getResource(path: string)`: gets a resource at a given path. [See API](https://deno.land/x/windmill/mod.ts?s=getResource).

The states can be seen in the Resources section of Windmill app with a
[Resource Type](../3_resources_and_types/index.mdx#create-a-resource-type) of `state`.

:::tip

Variables are similar to resources but have no types, can be tagged as `secret` (in which case they are encrypted by the workspace key) and can only store strings. In some situations, you may prefer `setVariable`/`getVariable` to resources.
:::

In conclusion `setState` and `setResource` are convenient ways to persist json between multiple script executions.

<div class="grid grid-cols-2 gap-6 mb-4">
	<DocCard
		title="Resources and Resource Types"
		description="Resources are structured configurations and connections to third-party systems, with Resource Types defining the schema for each Resource."
		href="/docs/core_concepts/resources_and_types"
	/>
</div>

### Shared Directory

For heavier ETL processes or sharing data between steps in a flow, Windmill provides a [Shared Directory](../../flows/3_editor_components.mdx#shared-directory) feature.

The Shared Directory allows steps within a flow to share data by storing it in a designated folder.

:::caution

Although Shared Folders are recommended for persisting states within a flow, it's important to note that all steps are executed on the same worker and the data stored in the Shared Directory is strictly ephemeral to the flow execution.

:::

To enable the Shared Directory, follow these steps:

1. Open the `Settings` menu in the Windmill interface.
2. Go to the `Shared Directory` section.
3. Toggle on the option for `Shared Directory on './shared'`.

![Flow Shared Directory](../../assets/flows/flow_settings_shared_directory.png.webp)

Once the Shared Directory is enabled, you can use it in your flow by referencing the `./shared` folder. This folder is shared among the steps in the flow, allowing you to store and access data between them.

:::tip

Keep in mind that the contents of the `./shared` folder are not preserved across [suspends](../../flows/11_flow_approval.mdx) and [sleeps](../../flows/15_sleep.md). The directory is temporary and active only during the execution of the flow.

:::

## Structured Databases: Postgres (Supabase, Neon.tech)

For Postgres databases (best for structured data storage and retrieval, where you can define schema and relationships between entities), we recommend using Supabase or Neon.tech.

### Supabase

[Supabase](https://supabase.com/) is an open-source alternative to Firebase, providing a backend-as-a-service platform that offers a suite of tools, including real-time subscriptions, authentication, storage, and a PostgreSQL-based database.

1. Sign-up to Supabase's <a href="https://app.supabase.com/sign-up" rel="nofollow" >Cloud App</a> or [Self-Host](https://supabase.com/docs/guides/self-hosting) it.

2. [Create a new Supabase project](https://supabase.com/docs/guides/getting-started).

3. Get a [Connection string](https://supabase.com/docs/guides/database/connecting-to-postgres#finding-your-connection-string).

   - Go to the `Settings` section.
   - Click `Database`.
   - Find your Connection Info and Connection String. Direct connections are on port 5432.

4. From Windmill, add your Supabase connection string as a [Postgresql resource](https://hub.windmill.dev/resource_types/114/postgresql) and [Execute queries](https://hub.windmill.dev/scripts/postgresql/1294/execute-query-and-return-results-postgresql). Tip: you might need to set the `sslmode` to "disable".

<video
	className="border-2 rounded-xl object-cover w-full h-full dark:border-gray-800"
	controls
	src="/videos/supabase_postgres_integration.mp4"
/>

<br />

You can also integrate Supabase [directly through its API](../../integrations/supabase.md#through-supabase-api).

:::tip

You can find examples and premade Supabase scripts on [Windmill Hub](https://hub.windmill.dev/integrations/supabase).

<br />

More tutorials on Supabase:

- [How to Send Database Events From Supabase to Windmill](/blog/database-events-from-supabase-to-windmill)
- [Create an E-commerce backoffice](../../apps/7_app_e-commerce.md)
- [Create an Issue Tracker App with Supabase in 15 Minutes](/blog/create-issue-tracker-in-15-minutes)
- [Create an Issue Tracker App with Supabase - Part 2 Customize Your App](/blog/create-issue-tracker-part-2)
- [Use Supabase Authentication on Windmill to query RLS protected tables for external apps](/blog/supabase-authentication-and-rls-protected-tables-on-windmill)

:::

### Neon.tech

[Neon.tech](https://neon.tech/) is an open-source cloud database platform that provides fully managed PostgreSQL databases with high availability and scalability.

1. Sign-up to Neon's <a href="https://console.neon.tech/sign_in" rel="nofollow" >Cloud App</a> or [Self-Host](https://community.neon.tech/t/can-neon-be-self-hosted/51) it.

2. [Set up a project and add data](https://neon.tech/docs/manage/projects).

3. Get a [Connection string](https://neon.tech/docs/connect/query-with-psql-editor). You can obtain it connection string from the Connection Details widget on the Neon Dashboard: select a branch, a role, and the database you want to connect to and a connection string will be constructed for you.

4. From Windmill, add your Neon.tech connection string as a [Postgresql resource](https://hub.windmill.dev/resource_types/114/postgresql) and [Execute queries](https://hub.windmill.dev/scripts/postgresql/1294/execute-query-and-return-results-postgresql).

<video
	className="border-2 rounded-xl object-cover w-full h-full dark:border-gray-800"
	controls
	src="/videos/neon_integration.mp4"
/>

<br />

:::tip

Adding the connection string as a Postgres resource requires to parse it.

<br />

For example, for `psql postgres://daniel:<password>@ep-restless-rice.us-east-2.aws.neon.tech/neondb`, that would be:

```json
{
	"host": "ep-restless-rice.us-east-2.aws.neon.tech",
	"port": 5432,
	"user": "daniel",
	"dbname": "neondb",
	"sslmode": "require",
	"password": "<password>"
}
```

Where the sslmode should be "require" and Neon uses the default PostgreSQL port, `5432`.

:::

## Large Data Files: S3, R2, MinIO

On heavier data objects & unstructured data storage, Amazon S3 (Simple Storage Service) and its alternatives Cloudflare R2 and MinIO are highly scalable and durable object storage service that provides secure, reliable, and cost-effective storage for a wide range of data types and use cases.

### Use Amazon S3, R2 and MinIO directly

Amazon S3, Cloudflare R2 and MinIO all follow the same API schema and therefore have a [common Windmill resource type](https://hub.windmill.dev/resource_types/42/).

#### Amazon S3

[Amazon S3](https://aws.amazon.com/s3/) (Simple Storage Service) is a scalable and durable object storage service offered by Amazon Web Services (AWS), designed to provide developers and businesses with an effective way to store and retrieve any amount of data from anywhere on the web.

<video
	className="border-2 rounded-xl object-cover w-full h-full dark:border-gray-800"
	controls
	src="/videos/s3_objects_in_bucket.mp4"
/>

<br />

1. [Sign-up to AWS](https://aws.amazon.com/resources/create-account/).

2. [Create a bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html) on S3.

3. [Integrate it to Windmill](../../integrations/aws-s3.md) by filling the [resource type details](https://hub.windmill.dev/resource_types/42) for S3 APIs.

Make sure the user associated with the resource has the [right policies allowed](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html) in AWS Identity and Access Management (IAM).

:::tip

You can find examples and premade S3 scripts on [Windmill Hub](https://hub.windmill.dev/integrations/s3).

:::

#### Cloudflare R2

[Cloudflare R2](https://www.cloudflare.com/products/r2/) is a cloud-based storage service that provides developers and businesses with a cost-effective and secure way to store and access their data.

1. [Sign-up to Cloudflare](https://developers.cloudflare.com/fundamentals/account-and-billing/account-setup/create-account/)

2. [Create a bucket](https://developers.cloudflare.com/r2/get-started/) on R2.

3. [Integrate it to Windmill](../../integrations/cloudflare-r2.md) by filling the [resource type details](https://hub.windmill.dev/resource_types/42) for S3 APIs.

#### MinIO

For best performance, [install MinIO locally](https://min.io/docs/minio/kubernetes/upstream/).

[MinIO](https://min.io/) is an open-source, high-performance, and scalable object storage server that is compatible with Amazon S3 APIs, designed for building private and public cloud storage solutions.

Then from Windmill, just [fill the S3 resource type](../../integrations/s3.md).

#### Windmill code snippets

<Tabs className="unique-tabs">
<TabItem value="deno" label="TypeScript (Deno)" attributes={{className: "text-xs p-4 !mt-0 !ml-0"}}>

```ts
import * as wmill from 'npm:windmill-client@1';
import { S3Client } from 'https://deno.land/x/s3_lite_client@0.2.0/mod.ts';

type s3object = object;

export async function main(inputFile: s3object) {
	const s3Resource = await wmill.getResource('<PATH_TO_S3_RESOURCE>');
	const s3Client = new S3Client(s3Resource);
	const outputFile = 'output/hello.txt';

	// read object from S3
	const getObjectResponse = await s3Client.getObject(inputFile['s3']);
	const inputObjContent = await getObjectResponse.text();
	console.log(inputObjContent);

	// write object to S3
	await s3Client.putObject(outputFile, 'Hello Windmill!');

	// list objects from bucket
	for await (const obj of s3Client.listObjects({ prefix: 'output/' })) {
		console.log(obj.key);
	}

	return {
		s3: outputFile
	};
}
```

</TabItem>
<TabItem value="python" label="Python" attributes={{className: "text-xs p-4 !mt-0 !ml-0"}}>

```python
import wmill
import boto3

s3object = dict


def main(input_file: s3object):
    s3_resource = wmill.get_resource("<PATH_TO_S3_RESOURCE>")
    bucket = s3_resource["bucket"]
    s3client = boto3.client(
        "s3",
        region_name=s3_resource["region"],
        aws_access_key_id=s3_resource["accessKey"],
        aws_secret_access_key=s3_resource["secretKey"],
    )
    output_file = "output/hello.txt"

    # read object from S3 and print its content
    input_obj = s3client.get_object(Bucket=bucket, Key=input_file["s3"])["Body"].read()
    print(input_obj)

    # write object to s3
    s3client.put_object(Bucket=bucket, Key=output_file, Body="Hello Windmill!")

    # download file to the job temporary folder:
    s3client.download_file(
        Bucket=bucket, Key=input_file["s3"], Filename="./download.txt"
    )
    with open("./download.txt", mode="rb") as downloaded_file:
        print(downloaded_file.read())

    # upload file from temporary folder to S3
    uploaded_file = "output/uploaded.txt"
    with open("./upload.txt", mode="wb") as file_to_upload:
        file_to_upload.write(str.encode("Hello Windmill!"))
    s3client.upload_file(Bucket=bucket, Key=uploaded_file, Filename="./upload.txt")

    # see https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html
    # and https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html
    # for more code examples (listing object, deleting files, etc)

    return [
        s3object({"s3": output_file}),
        s3object({"s3": uploaded_file}),
    ]
```

</TabItem>
</Tabs>

### Windmill embedded integration with S3, Polars and DuckDB for data pipelines

Run your ETLs on-prem up to 5x faster using Windmill compared to Spark while simplifying your infra.

You can link a Windmill workspace to an S3 bucket and use it as source and/or target of your processing steps seamlessly, without any boilerplate.

<Tabs className="unique-tabs">
<TabItem value="polars" label="Polars" attributes={{className: "text-xs p-4 !mt-0 !ml-0"}}>

```python
import wmill
import polars as pl
import s3fs

s3object = dict


def main(input_file: s3object):
    s3 = s3fs.S3FileSystem(
        # this will default to the workspace s3 resource
        **wmill.polars_connection_settings()["s3fs_args"]
        # this will use the designated resource
        # **wmill.polars_connection_settings("<PATH_TO_S3_RESOURCE>")["s3fs_args"]
    )

    bucket = "<S3_BUCKET_NAME>"
    input_uri = "s3://{}/{}".format(bucket, input_file["s3"])
    output_file = "output/result.parquet"
    output_uri = "s3://{}/{}".format(bucket, output_file)

    with (
        s3.open(input_uri, mode="rb") as input_s3,
        s3.open(output_uri, mode="wb") as output_s3,
    ):
        # input is a parquet file, we use read_parquet in lazy mode.
        # Polars can read various file types, see
        # https://pola-rs.github.io/polars/py-polars/html/reference/io.html
        input_df = pl.read_parquet(input_s3).lazy()

        # process the Polars dataframe. See Polars docs:
        # for dataframe: https://pola-rs.github.io/polars/py-polars/html/reference/dataframe/index.html
        # for lazy dataframe: https://pola-rs.github.io/polars/py-polars/html/reference/lazyframe/index.html
        output_df = input_df.collect()
        print(output_df)

        # persist the output dataframe back to S3 and return it
        output_df.write_parquet(output_s3)
    return s3object({"s3": output_file})
```

</TabItem>
<TabItem value="duckdb" label="DuckDB" attributes={{className: "text-xs p-4 !mt-0 !ml-0"}}>

```python
import wmill
import duckdb

s3object = dict


def main(input_file: s3object):
    # create a DuckDB database in memory
    # see https://duckdb.org/docs/api/python/dbapi
    conn = duckdb.connect()
    # connect duck db to the S3 bucket - this will default to the workspace s3 resource
    conn.execute(wmill.duckdb_connection_settings()["connection_settings_str"])
    # this will use the designated resource
    # conn.execute(wmill.duckdb_connection_settings("<PATH_TO_S3_RESOURCE>")["connection_settings_str"])

    bucket = "<S3_BUCKET_NAME>"
    input_uri = "s3://{}/{}".format(bucket, input_file["s3"])
    output_file = "output/result.parquet"
    output_uri = "s3://{}/{}".format(bucket, output_file)

    # Run queries directly on the parquet file
    query_result = conn.sql(
        """
        SELECT * FROM read_parquet('{}')
    """.format(
            input_uri
        )
    )
    query_result.show()

    # Write the result of a query to a different parquet file on S3
    conn.execute(
        """
        COPY (
            SELECT COUNT(*) FROM read_parquet('{input_uri}')
        ) TO '{output_uri}' (FORMAT 'parquet');
    """.format(
            input_uri=input_uri, output_uri=output_uri
        )
    )

    conn.close()
    return s3object({"s3": output_file})
```

</TabItem>
</Tabs>

For more info, see our page dedicated to Data Pipelines in Windmill:

<div class="grid grid-cols-2 gap-6 mb-4">
	<DocCard
		title="Data Pipelines"
		description="We have integrated with Polars and DuckDB for in-memory data processing and S3 for external storage."
		href="/docs/core_concepts/data_pipelines"
	/>
</div>

## Key-Value Stores: MongoDB Atlas, Redis, Upstash

Key-value stores are a popular choice for managing non-structured data, providing a flexible and scalable solution for various data types and use cases. In the context of Windmill, you can use MongoDB Atlas, Redis, and Upstash to store and manipulate non-structured data effectively.

### MongoDB Atlas

[MongoDB Atlas](https://www.mongodb.com/atlas/database) is a managed database-as-a-service platform that provides an efficient way to deploy, manage, and optimize MongoDB instances. As a document-oriented NoSQL database, MongoDB is well-suited for handling large volumes of unstructured data. Its dynamic schema enables the storage and retrieval of JSON-like documents with diverse structures, making it a suitable option for managing non-structured data.

To use MongoDB Atlas with Windmill:

1. [Sign-up to Atlas](https://www.mongodb.com/cloud/atlas/signup).

2. [Create a database](https://www.mongodb.com/basics/create-database).

3. [Integrate it to Windmill](../../integrations/mongodb.md) by filling the [resource type details](https://hub.windmill.dev/resource_types/22/).

:::tip

You can find examples and premade MonggoDB scripts on [Windmill Hub](https://hub.windmill.dev/integrations/mongodb).

:::

### Redis

[Redis](https://redis.io/) is an open-source, in-memory key-value store that can be used for caching, message brokering, and real-time analytics. It supports a variety of data structures such as strings, lists, sets, and hashes, providing flexibility for non-structured data storage and management. Redis is known for its high performance and low-latency data access, making it a suitable choice for applications requiring fast data retrieval and processing.

To use Redis with Windmill:

1. [Sign-up to Redis](https://redis.com/try-free/).

2. [Create a database](https://developer.redis.com/create).

3. [Integrate it to Windmill](../../integrations/redis.md) by filling the [resource type details](https://hub.windmill.dev/resource_types/22/) following the same schema as MongoDB Atlas.

### Upstash

[Upstash](https://upstash.com/) is a serverless, edge-optimized key-value store designed for low-latency access to non-structured data. It is built on top of Redis, offering similar performance benefits and data structure support while adding serverless capabilities, making it easy to scale your data storage needs.

To use Upstash with Windmill:

1. [Sign-up to Upstash](https://console.upstash.com/).

2. [Create a database](https://docs.upstash.com/redis).

3. [Integrate it to Windmill](../../integrations/upstash.md) by filling the [resource type details](https://hub.windmill.dev/resource_types/22/) following the same schema as MongoDB Atlas.
